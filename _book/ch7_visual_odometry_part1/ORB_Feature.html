
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>ORB Feature Â· Computer Vision Learning Notes</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        <meta name="author" content="Shengchen Liu">
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="Feature_Matching.html" />
    
    
    <link rel="prev" href="Feature_Point.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    
    
        
        <li>
            <a href="https://shengchen-liu.github.io/notes" target="_blank" class="custom-link">Home</a>
        </li>
    
    

    
    <li class="divider"></li>
    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../ch2_first_glance/chapter2-first-glance-of-visual-SLAM.html">
            
                <a href="../ch2_first_glance/chapter2-first-glance-of-visual-SLAM.html">
            
                    
                    Chapter 2: First Glance at Visual SLAM
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../ch3_3D_rigid_body_motion/Chapter3_3D_Rigid_Body_Motion.html">
            
                <a href="../ch3_3D_rigid_body_motion/Chapter3_3D_Rigid_Body_Motion.html">
            
                    
                    Chapter 3: 3D Rigid Body Motion
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="../ch3_3D_rigid_body_motion/Chapter3_3D_Rigid_Body_Motion.html">
            
                <a href="../ch3_3D_rigid_body_motion/Chapter3_3D_Rigid_Body_Motion.html">
            
                    
                    Rotation Matrix
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="../ch3_3D_rigid_body_motion/3.3_Rotation_Vectors_and_Euler_Angles.html">
            
                <a href="../ch3_3D_rigid_body_motion/3.3_Rotation_Vectors_and_Euler_Angles.html">
            
                    
                    Rotation Vector and Euler Angle
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="../ch3_3D_rigid_body_motion/3.3_Quaternions.html">
            
                <a href="../ch3_3D_rigid_body_motion/3.3_Quaternions.html">
            
                    
                    Quaternions
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.4" data-path="../ch3_3D_rigid_body_motion/3.4_Affine_and_Projective_Transformation.html">
            
                <a href="../ch3_3D_rigid_body_motion/3.4_Affine_and_Projective_Transformation.html">
            
                    
                    Affine and Projective Transformation
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="Visual_Odometry_Part_1.html">
            
                <a href="Visual_Odometry_Part_1.html">
            
                    
                    Chapter 7: Visual Odometry: Feature Point
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="Feature_Point.html">
            
                <a href="Feature_Point.html">
            
                    
                    Feature Point
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.4.2" data-path="ORB_Feature.html">
            
                <a href="ORB_Feature.html">
            
                    
                    ORB Feature
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.3" data-path="Feature_Matching.html">
            
                <a href="Feature_Matching.html">
            
                    
                    Feature Matching
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4" data-path="Calculate_Camera_Motion.html">
            
                <a href="Calculate_Camera_Motion.html">
            
                    
                    Calculate Camera Motion
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.4.1" data-path="2D-2D_epipolar_geometry.html">
            
                <a href="2D-2D_epipolar_geometry.html">
            
                    
                    2D-2D: epipolar geometry
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4.2" data-path="3D-2DPnP.html">
            
                <a href="3D-2DPnP.html">
            
                    
                    3D-2D: PnP
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4.3" data-path="3D-3D_ICP.html">
            
                <a href="3D-3D_ICP.html">
            
                    
                    3D-3D: ICP
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >ORB Feature</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="orb-feature">ORB Feature</h1>
<p>ORB features are composed of two parts: key points and descriptor. </p>
<p>Two steps:</p>
<ul>
<li><strong>Key point</strong>: <strong>Oriented FAST</strong>. Compared with the original FAST, the main direction of the feature points is calculated</li>
<li><strong>Descriptor: BRIEF</strong> (Binary Robust Independent Elementary Feature) .  Describe the surrounding image area where the feature points were extracted in the previous step.  ORB has made some improvements to BRIEF, mainly referring to utilizing the previously calculated direction</li>
</ul>
<h2 id="1-fast-key-points">1. FAST Key points</h2>
<p>FAST is a kind of corner point, which mainly detects the obvious grayscale changes locally, and is known for its fast speed.</p>
<h3 id="11-main-idea">1.1 Main Idea</h3>
<p>If a pixel is very different from the neighboring pixels (too bright or too dark), then it is more likely to be a corner points. </p>
<p>Compared with other corner detection algorithms, FAST only needs to compare the brightness of the pixels, which is very fast.</p>
<h3 id="12-entire-procedure">1.2 Entire Procedure</h3>
<ol>
<li>Select pixel <script type="math/tex; ">p</script> in the image assuming its brightness as <script type="math/tex; ">I_p</script></li>
<li><p>Set a threshold <script type="math/tex; ">T</script> (for example, 20% of <script type="math/tex; ">I_p</script>)</p>
</li>
<li><p>Take the pixel <script type="math/tex; ">p</script> as the center, and select the 16 pixels on a circle with a radius of 3.</p>
</li>
<li>If there are consecutive N points on the selected circle whose brightness is greater than <script type="math/tex; ">I_p + T</script> or less than <script type="math/tex; ">I_p - T</script>, then the central pixel <script type="math/tex; ">p</script> can be considered as a feature point (<script type="math/tex; ">N</script> usually takes 12, which is FAST-12).</li>
<li>Iterate though the above four steps on each pixel.</li>
</ol>
<h3 id="13-good">1.3 Good</h3>
<ul>
<li><p>In the FAST-12 algorithm, to speed up, checking the brightness of the 1, 5, 9, and 13-th pixels on the circle for each pixel can quickly exclude a lot of pixels that are not corner points.  Only when 3 out of 4 pixels are greater than <script type="math/tex; ">I_p + T</script> or less than <script type="math/tex; ">I_p - T</script>, may the current pixel be a corner points, otherwise it should be excluded directly.  Such &apos;pre-processing&apos; operation greatly accelerates FAST corner detection.</p>
</li>
<li><p>Non-maximal suppression:  the original FAST corners are often &#x201D;clustered&#x201D;, meaning a lot of FAST corners present in the same area. Therefore, after the initial detection, non-maximal suppression is required. Only corner points with maximum response in a certain area will be retained to avoid the corners concentrating.</p>
</li>
</ul>
<p><img src="../home/shengchen/gitbook/visual_slam_gitbook/ch7_visual_odometry_part1/asset/fast-corner.png" alt=""></p>
<h3 id="14-bad">1.4 Bad</h3>
<ul>
<li>The calculation of FAST feature points only compares the brightness difference between pixels, thus the speed is very fast, but it suffers from bad repeatability and uneven distribution. </li>
<li>FAST corner points do not include direction information. </li>
<li>It fixed the radius of circle as 3, there is also a scaling problem: a place that looks like a corner from a distance may not be a corner when it comes close. </li>
</ul>
<h2 id="2-oriented-fast-key-points">2. Oriented FAST Key points</h2>
<p>ORB adds the description of scale and rotation. The <strong>scale invariance</strong> is achieved by the image pyramid, and detect corner points on each layer of the pyramid. The <strong>rotation</strong> of features is realized by the Intensity Centroid method.</p>
<h3 id="21-image-pyramid">2.1 Image Pyramid</h3>
<p>Pyramid is a common approach in computer vision.  The bottom of the pyramid is the original image. For each layer up, the image is scaled with a fixed ratio, so that we have images of different resolutions. The smaller image can be seen as a scene viewed from a distance. In the feature matching
algorithm, we can match images on different layers to achieve scale invariance.</p>
<h3 id="22-intensity-centroid">2.2 Intensity Centroid</h3>
<p>In terms of rotation, we calculate the gray centroid of the image near the feature point.  The so-called centroid refers to the gray value of the image block as the center of weight. The specific steps are as follows</p>
<ol>
<li>In a small image block <script type="math/tex; ">B</script>, define the moment of the image block as</li>
</ol>
<p><script type="math/tex; mode=display">
m_{pq}=\sum_{x,y \in B}x^{p}y^{q}I(x,y), \quad p, q = \{0,1\}.
</script></p>
<ol>
<li>Calculate the centroid of the image block by the moment:</li>
</ol>
<p><script type="math/tex; mode=display">
C=\left(\frac{m_{10}}{m_{00}},\frac{m_{01}}{m_{00}}\right).
</script></p>
<ol>
<li>Connect the geometric center <script type="math/tex; ">O</script> and the centroid <script type="math/tex; ">C</script> of the image block to get a direction vector <script type="math/tex; ">\overrightarrow{OC}</script>, so the direction of the feature point can be defined as</li>
</ol>
<p><script type="math/tex; mode=display">
\theta = \arctan(m_{01}/m_{10}).
</script></p>
<h2 id="3-brief-descriptor">3. BRIEF Descriptor</h2>
<p>After extracting the Oriented FAST key points, we calculate the descriptor for each point. ORB uses an improved BRIEF descriptor.</p>
<h3 id="31-brief">3.1 BRIEF</h3>
<p>BRIEF is a binary descriptor.  Its description vector consists of many 0s and 1s, where 0s and 1s encode the size relationship between two random pixels near the key point (such as <script type="math/tex; ">p</script> and <script type="math/tex; ">q</script>): If <script type="math/tex; ">p</script> is greater than <script type="math/tex; ">q</script>, then take 1, otherwise take 0. If wetake 128 such p, q pairs, we will finally get a 128-dimensional vector consisting of 0s and 1s.</p>
<p><strong>Good</strong>:</p>
<p>BRIEF implements the comparison of randomly selected points, which is very fast, and since it expresses in binary, it is also very convenient to store and
suitable for real-time image matching.</p>
<p><strong>Bad</strong>:</p>
<p>The original BRIEF descriptor does not have rotation invariance, so it is easy to get lost when the image is rotated.</p>
<h3 id="32-improved-brief">3.2 Improved BRIEF</h3>
<p>The ORB calculates the direction of the key points in the FAST feature point extraction stage, so the direction information can be used to calculate the &#x201D;Steer BRIEF&#x201D; feature after the rotation, so that the ORB descriptor has better rotation invariance.</p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="Feature_Point.html" class="navigation navigation-prev " aria-label="Previous page: Feature Point">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="Feature_Matching.html" class="navigation navigation-next " aria-label="Next page: Feature Matching">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"ORB Feature","level":"1.4.2","depth":2,"next":{"title":"Feature Matching","level":"1.4.3","depth":2,"path":"ch7_visual_odometry_part1/Feature_Matching.md","ref":"ch7_visual_odometry_part1/Feature_Matching.md","articles":[]},"previous":{"title":"Feature Point","level":"1.4.1","depth":2,"path":"ch7_visual_odometry_part1/Feature_Point.md","ref":"ch7_visual_odometry_part1/Feature_Point.md","articles":[]},"dir":"ltr"},"config":{"plugins":["mathjax"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"mathjax":{"forceSVG":false,"version":"2.6-latest"},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"theme":"default","author":"Shengchen Liu","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"Computer Vision Learning Notes","links":{"sidebar":{"Home":"https://shengchen-liu.github.io/notes"}},"gitbook":"*","description":"A collection of learning notes of Visual SLAM"},"file":{"path":"ch7_visual_odometry_part1/ORB_Feature.md","mtime":"2021-03-05T18:51:30.042Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2021-04-22T03:36:24.775Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

